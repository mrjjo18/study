{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6d21f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:22:42.120837Z",
     "start_time": "2021-08-03T15:22:41.995912Z"
    }
   },
   "source": [
    "![python image2](https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff9400",
   "metadata": {},
   "source": [
    "# 1. RNN?\n",
    "  1. 내부적으로 순환(Recurrent)되는 구조를 이용하여\n",
    "  2. 순서(Sequence)가 있는 데이터를 처리하는 데 강점을 가진 NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faab603",
   "metadata": {},
   "source": [
    "## 1.1 순서가 있는 데이터\n",
    "- 문장이나 음성, 시계열 데이터처럼 Sequence에 따른 데이터\n",
    "- 예를 들어, I work at google / I google at work 두 문장이 있을 때, 동사로 활용된 work 또는 google을 동사로 제대로 해석하려면 첫 번째 단어가 주어 'I'라는 것을 메모리 상에 기억하고 있어야함. 또한 I, work(or google)까지 기억한다면 at google과 at work를 올바르게 해석하여 결국 문장 전체를 올바르게 해석할 수 있음.  \n",
    "<br><br>\n",
    "  \n",
    "**RNN은**\n",
    "  1. 은닉층 내의 순환구조를 이용하여 메모리 cell이 각 sequence의 데이터를 기억하고 있다가, \n",
    "  2. 새롭게 입력으로 주어지는 데이터(입력)와 메모리 cell이 옆 층으로 전달해준 과거의(t-1로 가정) 데이터를 연결시켜 출력값(의미)을 알아내는 기능을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86821ab",
   "metadata": {},
   "source": [
    "## 1.2 DNN Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b330c3f",
   "metadata": {},
   "source": [
    "![python image2](image_source/DNN_principle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c2071",
   "metadata": {},
   "source": [
    "## 1.3. RNN 구조\n",
    "\n",
    "![python image2](image_source/RNN_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49bfd3",
   "metadata": {},
   "source": [
    "## 1.4 RNN 동작 원리 - 1\n",
    "\n",
    "![python image2](image_source/RNN_principle_00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b101ae",
   "metadata": {},
   "source": [
    "![python image2](image_source/RNN_principle_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc320a0c",
   "metadata": {},
   "source": [
    "1. 'I'의 품사: (t1) 참고할만한 과거의 data가 없어서\n",
    "2. 'work'의 품사: (t2) 은닉층으로 들어오긴 하는데, t1시점의 출력값을 I(주어)를 우리가 기억하고 있으므로 I를 활용해 은닉층 계산을 하고 출력층에서 가장 확률이 높은 품사 기준으로 값 출력\n",
    "3. 'at'의 품사: (t3) t1 시점의 I, 그리고 t1시점 I의 영향을 받은 t2 시점의 work 출력값을 기준(softmax)으로 출력한다.\n",
    "4. 'google'의 품사: (t4) t1, t2, t3 I, work, at를 토대로 주어, 동사, 전치사 다음에는 명사가 올 확률이 높기 때문에 softmax기준으로 명사로 분류\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5050e39",
   "metadata": {},
   "source": [
    "## 1.5 RNN 동작원리 - 2\n",
    "![python image2](image_source/RNN_principle_02.png)\n",
    "\n",
    "첫 번째 데이터라 $H_{cur} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092d151",
   "metadata": {},
   "source": [
    "## 1.6 RNN 동작원리 - 3\n",
    "\n",
    "![python image2](image_source/RNN_principle_03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400ccd12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.691092Z",
     "start_time": "2021-08-09T01:38:04.592350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               29312     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 529,441\n",
      "Trainable params: 529,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Embedding, Dense\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(SimpleRNN(hidden_size))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548d1e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.702641Z",
     "start_time": "2021-08-09T01:38:09.693100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529441"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000 * 100 + 100*128 + 128*128 + 128 + 128 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737c494",
   "metadata": {},
   "source": [
    "# 2. 장단기 메모리(Long Short-Term Memory, LSTM)\n",
    "- 앞서 배운 RNN(vanilla)의 한계를 극복하기 위한 변형\n",
    "\n",
    "![python image](https://wikidocs.net/images/page/22888/lstm_image1_ver2.PNG)\n",
    "\n",
    "음영을 정보량으로 가정했을 때 1에서 t로 갈수록 음영이 옅어지는 것을 볼 수 있음. 이는 곧 과거의 정보량이 점차 소실되어가는 과정을 표현한 것.\n",
    "\n",
    "예)\n",
    "  - 모스크바에 여행을 왔는데, 건물도 예쁘고 음식도 맛있었어. 그런데 직장 상사한테 전화가 왔어 어디냐고 묻더라고~ (중량)\n",
    "  - 만약 이 문장을 임베딩해서 다음 단어를 예측한다라고 했을 때, 모스크바라는 벡터의 정보량이 소실된다면 예측에 어떤 영향을 줄 것인가?\n",
    "  \n",
    " ***= 장기 의존성 문제(the problem of Long-Term Dependencies)***\n",
    " \n",
    " \n",
    " \n",
    "**그림이 뭔가 어렵네요!**\n",
    " ![python image](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG)\n",
    " \n",
    " \n",
    " 각각의 요소에 대한 설명도 읽어보았으나... 넘어가야할 것 같네요..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe7998",
   "metadata": {},
   "source": [
    "# 3. 게이트 순환 유닛(Gated Recurrent Unit, GRU)\n",
    "- GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였습니다. 다시 말해서, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시켰습니다.\n",
    "\n",
    "![python image2](https://wikidocs.net/images/page/22889/GRU.PNG)\n",
    "\n",
    "  \n",
    "- LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했습니다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재합니다.\n",
    "- 데이터 양이 적을 때는, 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고 알려져 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43572a0",
   "metadata": {},
   "source": [
    "# 4. 케라스의 SimpleRNN과 LSTM 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ff7f4",
   "metadata": {},
   "source": [
    "## 4.1 임의의 입력 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caecfb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.707600Z",
     "start_time": "2021-08-09T01:38:09.705093Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Bidirectional\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e260ae82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.713962Z",
     "start_time": "2021-08-09T01:38:09.709634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1, 4.2, 1.5, 1.1, 2.8],\n",
      " [1.0, 3.1, 2.5, 0.7, 1.1],\n",
      " [0.3, 2.1, 1.5, 2.1, 0.1],\n",
      " [2.2, 1.4, 0.5, 0.9, 1.1]]\n",
      "(4, 5)\n",
      "5개의 단어 벡터를 사용한 4번의 기록(시점)\n"
     ]
    }
   ],
   "source": [
    "# 우선 RNN과 LSTM을 테스트하기 위한 임의의 입력을 만듭니다.\n",
    "\n",
    "train_X = [[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]\n",
    "pprint.pprint(train_X)\n",
    "print(np.shape(train_X))\n",
    "print(\"5개의 단어 벡터를 사용한 4번의 기록(시점)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141f40ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.720052Z",
     "start_time": "2021-08-09T01:38:09.716522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 RNN은 2D 텐서가 아니라 3D 텐서를 입력을 받는다고 언급한 바 있습니다. \n",
    "# 즉, 위에서 만든 2D 텐서를 3D 텐서로 변경합니다. 이는 배치 크기 1을 추가해주므로서 해결합니다.\n",
    "# 한 번 더 []로 싸서 차원추가\n",
    "train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]]\n",
    "train_X = np.array(train_X, dtype=np.float32)\n",
    "print(train_X.shape)\n",
    "#(batch_size, timesteps, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95eba4",
   "metadata": {},
   "source": [
    "## 4.2 SimpleRNN 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5ba0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.744104Z",
     "start_time": "2021-08-09T01:38:09.722119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[0.69865274 0.90117836 0.8384131 ]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(3)\n",
    "# rnn = SimpleRNN(3, return_sequences=False, return_state=False)와 동일.\n",
    "hidden_state = rnn(train_X)\n",
    "\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7b0736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.761313Z",
     "start_time": "2021-08-09T01:38:09.747011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states \n",
      ": [[[-0.8246665   0.96837324 -0.97853434]\n",
      "  [-0.822488    0.9941444  -0.9984675 ]\n",
      "  [ 0.6705411   0.9976432  -0.75334674]\n",
      "  [-0.5484653   0.8530429  -0.9942721 ]]], shape: (1, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# 기본적으로 return_sequences가 False인 경우에는 SimpleRNN은 마지막 시점의 은닉 상태만 출력합니다. \n",
    "# 이번에는 return_sequences를 True로 지정하여 모든 시점의 은닉 상태를 출력해봅시다.\n",
    "\n",
    "rnn = SimpleRNN(3, return_sequences=True)\n",
    "hidden_states = rnn(train_X)\n",
    "\n",
    "print('hidden states \\n: {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "# (batch_size, timesteps, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11f2e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.782877Z",
     "start_time": "2021-08-09T01:38:09.766093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[-0.9879279   0.98412365  0.36602896]], shape: (1, 3)\n",
      "last hidden state : [[-0.9879279   0.98412365  0.36602896]], shape: (1, 3)\n",
      "두 개의 출력 모두 마지막 시점의 은닉 상태를 출력하게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 그렇다면 return_sequences는 False인데, retun_state가 True인 경우는 어떨까요?\n",
    "\n",
    "rnn = SimpleRNN(3, return_sequences=False, return_state=True)\n",
    "hidden_state, last_state = rnn(train_X)\n",
    "\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\n",
    "\n",
    "print(\"두 개의 출력 모두 마지막 시점의 은닉 상태를 출력하게 됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c62af",
   "metadata": {},
   "source": [
    "## 4.3 LSTM 이해하기\n",
    "\n",
    "- 실제로 SimpleRNN이 사용되는 경우는 거의 없음\n",
    "\n",
    "- 이번에는 SimpleRNN 때와는 달리, 세 개의 결과를 반환합니다. return_sequences가 False이므로 우선 첫번째 결과는 마지막 시점의 은닉 상태입니다. 그런데 LSTM이 SimpleRNN과 다른 점은 return_state를 True로 둔 경우에는 마지막 시점의 은닉 상태뿐만 아니라 셀 상태까지 반환한다는 점입니다. 이번에는 return_sequences를 True로 바꿔보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ae51948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.806963Z",
     "start_time": "2021-08-09T01:38:09.785818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[ 0.02412647  0.20736653 -0.23971386]], shape: (1, 3)\n",
      "last hidden state : [[ 0.02412647  0.20736653 -0.23971386]], shape: (1, 3)\n",
      "last cell state : [[ 0.11198303  0.7572297  -0.47383654]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(3, return_sequences=False, return_state=True)\n",
    "hidden_state, last_state, last_cell_state = lstm(train_X)\n",
    "\n",
    "\n",
    "\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\n",
    "print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb4d70",
   "metadata": {},
   "source": [
    "## 4.4 Bidirectional(LSTM) 이해하기(양방향순환)\n",
    "\n",
    "\n",
    "\n",
    "난이도를 조금 올려서 양방향 LSTM의 출력값을 확인해보겠습니다. return_sequences가 True인 경우와 False인 경우에 대해서 은닉 상태의 값이 어떻게 바뀌는지 직접 비교하기 위해서 이번에는 출력되는 은닉 상태의 값을 고정시켜주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a247cd4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.812292Z",
     "start_time": "2021-08-09T01:38:09.809328Z"
    }
   },
   "outputs": [],
   "source": [
    "k_init = tf.keras.initializers.Constant(value=0.1)\n",
    "b_init = tf.keras.initializers.Constant(value=0)\n",
    "r_init = tf.keras.initializers.Constant(value=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f63a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.853475Z",
     "start_time": "2021-08-09T01:38:09.815704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[0.6303138 0.6303138 0.6303138 0.7038734 0.7038734 0.7038734]], shape: (1, 6)\n",
      "forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n",
      "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1b56",
   "metadata": {},
   "source": [
    "이번에는 무려 5개의 값을 반환합니다. return_state가 True인 경우에는 정방향 LSTM의 은닉 상태와 셀 상태, 역방향 LSTM의 은닉 상태와 셀 상태 4가지를 반환하기 때문입니다. 다만, 셀 상태는 각각 forward_c와 backward_c에 저장만 하고 출력하지 않았습니다. 첫번째 출력값의 크기가 (1, 6)인 것에 주목합시다. 이는 return_sequences가 False인 경우 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태가 연결된 채 반환되기 때문입니다. 그림으로 표현하면 아래와 같이 연결되어 다음층에서 사용됩니다.\n",
    "\n",
    "![python image2](https://wikidocs.net/images/page/94748/bilstm3.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff7e0d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T01:38:09.892144Z",
     "start_time": "2021-08-09T01:38:09.855469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[0.35906473 0.35906473 0.35906473 0.7038734  0.7038734  0.7038734 ]\n",
      "  [0.55111325 0.55111325 0.55111325 0.58863586 0.58863586 0.58863586]\n",
      "  [0.59115744 0.59115744 0.59115744 0.3951699  0.3951699  0.3951699 ]\n",
      "  [0.6303138  0.6303138  0.6303138  0.21942244 0.21942244 0.21942244]]], shape: (1, 4, 6)\n",
      "forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n",
      "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, \\\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e20fed",
   "metadata": {},
   "source": [
    "hidden states의 출력값에서는 이제 모든 시점의 은닉 상태가 출력됩니다. 역방향 LSTM의 첫번째 시점의 은닉 상태는 더 이상 정방향 LSTM의 마지막 시점의 은닉 상태와 연결되는 것이 아니라 정방향 LSTM의 첫번째 시점의 은닉 상태와 연결됩니다.\n",
    "\n",
    "그림으로 표현하면 다음과 같이 연결되어 다음층의 입력으로 사용됩니다.\n",
    "\n",
    "![python image2](https://wikidocs.net/images/page/94748/bilstm1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958f296",
   "metadata": {},
   "source": [
    "# 5. RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)\n",
    "\n",
    "앞서 n-gram 언어 모델과 NNLM은 고정된 개수의 단어만을 입력으로 받아야한다는 단점이 있었습니다. 하지만 시점(time step)이라는 개념이 도입된 RNN으로 언어 모델을 만들면 입력의 길이를 고정하지 않을 수 있습니다. 이처럼 RNN으로 만든 언어 모델을 RNNLM(Recurrent Neural Network Language Model)이라고 합니다.\n",
    "\n",
    "\n",
    "아래 그림은 RNNLM이 어떻게 이전 시점의 단어들과 현재 시점의 단어로 다음 단어를 예측하는지를 보여줍니다.\n",
    "\n",
    "![python image2](https://wikidocs.net/images/page/46496/rnnlm1_final_final.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226c63a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe1f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
